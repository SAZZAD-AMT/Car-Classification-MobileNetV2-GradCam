{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WljBo-8jeQ8B"
      },
      "source": [
        "# **Description**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this notebook I'm use a PyTorch-implemented Gradient-weighted Class Activation Heatmap (Grad-CAM) for class activation exploration. \n",
        "\n",
        "I'l be using a fine-tuned MobileNetV2 (on Stanford Cars Dataset) as a model and torchcam as a Grad-CAM implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XfwC5y8U712y"
      },
      "outputs": [],
      "source": [
        "# Load dependies \n",
        "\n",
        "import os \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision.io.image import read_image\n",
        "from torchvision.transforms.functional import normalize, resize, to_pil_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0vu2B1kIlHep"
      },
      "outputs": [],
      "source": [
        "SAMPLES_PATH = \"samples/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "ozMRByz_9YGN",
        "outputId": "9dfffbac-f92f-465d-e360-4ba9bc231917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchcam in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.3.2)\n",
            "Requirement already satisfied: torch<2.0.0,>=1.7.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchcam) (1.12.1)\n",
            "Requirement already satisfied: Pillow>=8.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchcam) (8.4.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchcam) (3.5.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.14.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchcam) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (4.28.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->torchcam) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch<2.0.0,>=1.7.0->torchcam) (4.3.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.0.0->torchcam) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 21.3.1; however, version 22.2.2 is available.\n",
            "You should consider upgrading via the 'c:\\users\\hp\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install torchcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84AnSXqBAOUN",
        "outputId": "d6340101-9e58-4f4e-b788-126905bbf453"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/mob_netv2.pth'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Hp\\OneDrive\\Desktop\\Python Project\\Car-Classification-MobileNetV2-GradCam\\mobilenetv2_gradcam_visualization.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mmobilenet_v2()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mclassifier[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mclassifier[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39min_features, out_features\u001b[39m=\u001b[39m\u001b[39m196\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m/content/mob_netv2.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/mob_netv2.pth'"
          ]
        }
      ],
      "source": [
        "# Defining the model\n",
        "\n",
        "model = models.mobilenet_v2()\n",
        "model.classifier[1] = nn.Linear(in_features=model.classifier[1].in_features, out_features=196)\n",
        "model.load_state_dict(torch.load(\"/content/mob_netv2.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS9QFHjmBouR",
        "outputId": "be9a8334-aec9-49a5-bd93-991a8441c666"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:no value was provided for `target_layer`, thus set to 'features'.\n"
          ]
        }
      ],
      "source": [
        "from torchcam.methods import SmoothGradCAMpp\n",
        "from torchcam.utils import overlay_mask\n",
        "\n",
        "# Create CAM extractor\n",
        "\n",
        "cam_extractor = SmoothGradCAMpp(model.eval())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tCKalnntntym"
      },
      "outputs": [],
      "source": [
        "samples = []\n",
        "labels = []\n",
        "\n",
        "for im in os.listdir(SAMPLES_PATH):\n",
        "  samples.append(SAMPLES_PATH + im)\n",
        "  labels.append(im.split(\".\")[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUZnPXeRiZvE"
      },
      "source": [
        "# **Grad-CAM visualization**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "g2o7dHcqMP4w",
        "outputId": "6bf8c91e-e90b-494f-9de8-05be7d7b535e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Hp\\OneDrive\\Desktop\\Python Project\\Car-Classification-MobileNetV2-GradCam\\mobilenetv2_gradcam_visualization.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(nrows\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, ncols\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m15\u001b[39m), \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                         sharey\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, sharex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Plot Grad-CAM for each sample\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hp/OneDrive/Desktop/Python%20Project/Car-Classification-MobileNetV2-GradCam/mobilenetv2_gradcam_visualization.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, fig_x  \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ax\u001b[39m.\u001b[39mflatten()):\n",
            "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "fig, ax = plt.subplots(nrows=4, ncols=4,figsize=(15, 15), \\\n",
        "                        sharey=True, sharex=True)\n",
        "\n",
        "# Plot Grad-CAM for each sample\n",
        "\n",
        "for index, fig_x  in enumerate(ax.flatten()):\n",
        "\n",
        "    random_characters = int(np.random.uniform(0, len(samples)))\n",
        "    image = read_image(samples[index])\n",
        "    label = labels[index]\n",
        "    input_tensor = normalize(resize(image, (300, 300)) / 255., [1, 1, 1], [1, 1, 1])\n",
        "    out = model(input_tensor.unsqueeze(0))\n",
        "    \n",
        "    cam = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
        "    result = overlay_mask(to_pil_image(input_tensor), to_pil_image(cam[0], mode = \"F\"), alpha=0.5)\n",
        "    fig_x.imshow(result)\n",
        "    fig_x.set_title(label)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mobilenetv2_gradcam_visualization.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "4df2cc69eb3fd56a8a9780b07025dcfa15673fb6f5e64f079d5772d6ef5f08ae"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
